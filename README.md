# LLM Gateway

ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„å¤šAPIæä¾›å•†LLMä»£ç†æœåŠ¡ï¼Œæ”¯æŒOpenAIå…¼å®¹çš„APIæ¥å£ï¼Œæä¾›æ™ºèƒ½è´Ÿè½½å‡è¡¡ã€é€Ÿç‡é™åˆ¶ç®¡ç†å’Œå®æ—¶ç›‘æ§åŠŸèƒ½ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸš€ **å¤šAPIæä¾›å•†æ”¯æŒ**ï¼šæ”¯æŒå¤šä¸ªLLM APIæä¾›å•†ï¼Œè‡ªåŠ¨æ•…éšœè½¬ç§»
- âš¡ **æ™ºèƒ½è´Ÿè½½å‡è¡¡**ï¼šåŸºäºé€Ÿç‡é™åˆ¶å’Œå¯ç”¨æ€§è‡ªåŠ¨é€‰æ‹©æœ€ä½³æä¾›å•†
- ğŸ”’ **é€Ÿç‡é™åˆ¶ç®¡ç†**ï¼šæ”¯æŒRPMã€TPMã€RPDã€TPRç­‰å¤šç§é™åˆ¶ç­–ç•¥
- ğŸ“Š **å®æ—¶ç›‘æ§**ï¼šæä¾›APIä½¿ç”¨ç»Ÿè®¡å’Œå¥åº·æ£€æµ‹åŠŸèƒ½
- ğŸ¯ **OpenAIå…¼å®¹**ï¼šå®Œå…¨å…¼å®¹OpenAI APIæ¥å£è§„èŒƒ
- ğŸ³ **Dockeræ”¯æŒ**ï¼šæä¾›å®Œæ•´çš„Dockeréƒ¨ç½²æ–¹æ¡ˆ
- ğŸ”§ **Webç®¡ç†ç•Œé¢**ï¼šç›´è§‚çš„Webç•Œé¢è¿›è¡Œé…ç½®ç®¡ç†
- ğŸ“ˆ **æµå¼å“åº”**ï¼šæ”¯æŒæµå¼å“åº”å’Œå®æ—¶æ—¥å¿—è®°å½•

## å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python 3.10+
- Docker (å¯é€‰)

### å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### é…ç½®

1. å¤åˆ¶é…ç½®æ–‡ä»¶æ¨¡æ¿ï¼š
```bash
cp config.yaml.example config.yaml
```

2. ç¼–è¾‘ `config.yaml` æ–‡ä»¶ï¼Œé…ç½®æ‚¨çš„APIæä¾›å•†ä¿¡æ¯ï¼š

```yaml
api_provider:
  your_provider:
    base_url: https://api.example.com/v1
    api_key: your_api_key_here
    limits:
      rpm: 60    # æ¯åˆ†é’Ÿè¯·æ±‚æ•°
      tpm: 10000 # æ¯åˆ†é’Ÿtokenæ•°
      rpd: 1000  # æ¯æ—¥è¯·æ±‚æ•°
      tpr: 4000  # æ¯æ¬¡è¯·æ±‚æœ€å¤§tokenæ•°

model_config:
  gpt-4o:
    your_provider:
      alias: gpt-4o  # å¯é€‰ï¼Œå¦‚æœæä¾›å•†ä½¿ç”¨çš„æ¨¡å‹åç§°ä¸åŒ
      enable: true   # æ˜¯å¦å¯ç”¨è¯¥æä¾›å•†
```

### å¯åŠ¨æœåŠ¡

```bash
python app.py
```

æˆ–è€…ä½¿ç”¨Dockerï¼š

```bash
docker-compose up -d
```

æœåŠ¡å°†åœ¨ http://localhost:8100 å¯åŠ¨ã€‚

## APIä½¿ç”¨

### åŸºæœ¬ç”¨æ³•

```python
import openai

client = openai.OpenAI(
    base_url="http://localhost:8100/v1",
    api_key="any_key"  # ä»»æ„å€¼ï¼Œå®é™…APIå¯†é’¥åœ¨é…ç½®æ–‡ä»¶ä¸­é…ç½®
)

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Hello!"}],
    stream=False
)

print(response.choices[0].message.content)
```

### æµå¼å“åº”

```python
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Hello!"}],
    stream=True
)

for chunk in response:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="")
```

### è‡ªåŠ¨æ¨¡å‹é€‰æ‹©

```python
# ä½¿ç”¨"auto"æ¨¡å‹ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æ¨¡å‹
response = client.chat.completions.create(
    model="auto",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

## ç®¡ç†ç•Œé¢

è®¿é—® http://localhost:8100/admin æ‰“å¼€ç®¡ç†ç•Œé¢ï¼Œå¯ä»¥ï¼š

- æŸ¥çœ‹å’Œä¿®æ”¹æ¨¡å‹é…ç½®
- ç›‘æ§APIä½¿ç”¨æƒ…å†µ
- æ‰§è¡Œå¥åº·æ£€æµ‹
- é‡ç½®é€Ÿç‡é™åˆ¶
- æŸ¥çœ‹é”™è¯¯æ—¥å¿—

## é…ç½®è¯´æ˜

### APIæä¾›å•†é…ç½®

æ¯ä¸ªAPIæä¾›å•†éœ€è¦é…ç½®ä»¥ä¸‹å‚æ•°ï¼š

- `base_url`: APIåŸºç¡€URL
- `api_key`: APIå¯†é’¥
- `limits`: é€Ÿç‡é™åˆ¶é…ç½®
  - `rpm`: æ¯åˆ†é’Ÿè¯·æ±‚æ•°é™åˆ¶
  - `tpm`: æ¯åˆ†é’Ÿtokenæ•°é™åˆ¶
  - `rpd`: æ¯æ—¥è¯·æ±‚æ•°é™åˆ¶
  - `tpr`: æ¯æ¬¡è¯·æ±‚æœ€å¤§tokenæ•°é™åˆ¶

### æ¨¡å‹é…ç½®

æ¯ä¸ªæ¨¡å‹å¯ä»¥é…ç½®å¤šä¸ªæä¾›å•†ï¼š

- `alias`: æä¾›å•†ä½¿ç”¨çš„å®é™…æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
- `enable`: æ˜¯å¦å¯ç”¨è¯¥æä¾›å•†

## éƒ¨ç½²

### Dockeréƒ¨ç½²

```bash
# æ„å»ºé•œåƒ
docker-compose build

# å¯åŠ¨æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f
```

### ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

å»ºè®®ä½¿ç”¨åå‘ä»£ç†ï¼ˆå¦‚Nginxï¼‰å’Œè¿›ç¨‹ç®¡ç†å™¨ï¼ˆå¦‚PM2ï¼‰è¿›è¡Œç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ã€‚

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **APIè¿æ¥å¤±è´¥**ï¼šæ£€æŸ¥APIå¯†é’¥å’Œbase_urlé…ç½®
2. **é€Ÿç‡é™åˆ¶é”™è¯¯**ï¼šè°ƒæ•´limitsé…ç½®æˆ–æ·»åŠ æ›´å¤šAPIæä¾›å•†
3. **æ¨¡å‹ä¸å¯ç”¨**ï¼šç¡®ä¿æ¨¡å‹åœ¨æä¾›å•†å¤„å¯ç”¨ä¸”å·²å¯ç”¨

### æ—¥å¿—æŸ¥çœ‹

æ—¥å¿—æ–‡ä»¶ä½äº `log/` ç›®å½•ï¼š
- `app.log`: åº”ç”¨æ—¥å¿—
- `agent_interactions.log`: APIäº¤äº’æ—¥å¿—

## è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

## è®¸å¯è¯

MIT License

## æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æäº¤Issueæˆ–è”ç³»ç»´æŠ¤è€…ã€‚
